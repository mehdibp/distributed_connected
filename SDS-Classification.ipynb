{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689bc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import SDS_Environment as SDS\n",
    "from SDS_Environment import np, plt, tf, Camera, deque\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e616808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(env, N):\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # print first line in log file\n",
    "    \n",
    "#     f.close()\n",
    "    f = open(\"log_C.txt\", \"w\")\n",
    "    f.write(f\"episode \\t epsilon \\t \")\n",
    "    for i in range(N): f.write(f\"r{i} \\t \")\n",
    "    for i in range(N): f.write(f\"k{i} \\t \")\n",
    "    for i in range(N): f.write(f\"k{i}**2 \\t \")\n",
    "    for i in range(N): f.write(f\"k{i}**3 \\t \")\n",
    "    for i in range(N): f.write(f\"r{i}**2 \\t \")\n",
    "    for i in range(N): f.write(f\"fourth{i} \\t \")\n",
    "    for i in range(N): f.write(f\"H{i} \\t \")\n",
    "    for i in range(N): f.write(f\"reward{i} \\t \")\n",
    "    f.write(f\"Hamilton \\t\\t\")\n",
    "    for i in range(N): f.write(f\"All Q{i} \\t next Q{i} \\t Q{i} \\t target Q{i} \\t LOSS{i} \\t\")   ### \n",
    "    f.write(f\"\\t\")\n",
    "    for i in range(N):\n",
    "        for j in range(3): f.write(f\"Grads {i} {j} \\t bias {i} {j} \\t \")       ###\n",
    "    for i in range(N): \n",
    "        for j in range(3): f.write(f\"Trainable {i} {j} \\t bias {i} {j} \\t \")   ###\n",
    "        \n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # build models\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    input_shape = [3]              # == env.observation_space.shape\n",
    "    n_outputs = 2                  # == env.action_space.n\n",
    "\n",
    "    model = []                     # make a model for each agent\n",
    "    for i in range(N):\n",
    "#         initializer = tf.keras.initializers.GlorotNormal(seed = tf.random.set_seed(42+i)) # HeNormal()\n",
    "        model.append(tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "            tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "            tf.keras.layers.Dense(n_outputs, activation=custom_activation)\n",
    "        ]))\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # initilaze replay_memory & best_weights\n",
    "    replay_memory = []             # A bag containing 40 recently viewed items (state, action, reward, next_state)\n",
    "    best_weights  = []             # The best weights that each model will use to make the best decision\n",
    "    for i in range(N):\n",
    "        replay_memory.append(deque(maxlen=40))\n",
    "        best_weights .append(model[i].get_weights())\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # initilaze states and rewards from environment class    \n",
    "    state, reward = env.step(np.zeros(N), 1)\n",
    "    state = [[state[j][i] for j in range(input_shape[0])] for i in range(N)]\n",
    "    \n",
    "    return f, model, n_outputs, replay_memory, best_weights, state, reward\n",
    "    \n",
    "def custom_activation(x):\n",
    "    # return tf.keras.backend.switch(x >= 0, x**0.5, -(-x)**0.5)\n",
    "    xx = (tf.keras.backend.softplus(x))**0.5\n",
    "    return -tf.keras.backend.elu(-xx + 100) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a48df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_print(f, N, env, episode, epsilon, reward, H  ,  all_q, next_q, qq, target_q, LOSS, grads, trainable_variables):\n",
    "    \n",
    "    f.write(f\"\\n{episode} \\t {epsilon} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.r[i]} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.k[i]} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.k[i]**2} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.k[i]**3} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.r[i]**2} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.Hamiltonian(i)[1]} \\t \")\n",
    "    for i in range(N): f.write(f\"{env.Hamiltonian(i)[0]} \\t \")\n",
    "    for i in range(N): f.write(f\"{reward[i]} \\t \")\n",
    "    f.write(f\"{H} \\t\\t\")\n",
    "    \n",
    "    for i in range(N): \n",
    "        f.write(f\"{all_q[i].numpy().ravel()} \\t \")\n",
    "        f.write(f\"{next_q[i].ravel()} \\t \")\n",
    "        f.write(f\"{qq[i].numpy().ravel()} \\t \")\n",
    "        f.write(f\"{target_q[i].ravel()} \\t \")\n",
    "        f.write(f\"{LOSS[i]} \\t \")\n",
    "        \n",
    "    f.write(f\"\\t\")\n",
    "    np.set_printoptions(linewidth=np.inf)\n",
    "    for i in range(N):\n",
    "        for grad in grads[i]: f.write(f\"{grad.numpy().ravel()} \\t \")\n",
    "    for i in range(N):\n",
    "        for variable in trainable_variables[i]: f.write(f\"{variable.numpy().ravel()} \\t \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3b3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, N, model, state, n_outputs, replay_memory, epsilon=0.):\n",
    "    action = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action[i] = np.random.randint(n_outputs)\n",
    "        else:\n",
    "            actionnnn = model[0].predict(np.reshape(state[i], (3)).reshape(1, -1), verbose=0)[0]\n",
    "            action[i] = np.argmax(actionnnn)\n",
    "    \n",
    "    next_state, reward = env.step((action-0.5)*2 , epsilon)\n",
    "    next_state = [[next_state[j][i] for j in range(3)] for i in range(N)]\n",
    "    \n",
    "    for i in range(N):\n",
    "        replay_memory[i].append((state[i], action[i], reward[i], next_state[i]))\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32fff8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_H_G(episode, Hamilton, Giant):\n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(Hamilton)\n",
    "    plt.text(0.7*episode, 1.1*max(Hamilton), \"Min(H): %f\" % (min(Hamilton)) )\n",
    "    plt.text(0.7*episode, 1.2*max(Hamilton), \"Arg(H): %f\" % (np.argmin(Hamilton)) )\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(Giant)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb7168",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 100                      # Number of agents\n",
    "L = 15                       # The length of the simulation box\n",
    "env = SDS.Distributed_System(N,L)\n",
    "\n",
    "batch_size = 20\n",
    "discount_rate = 0.98\n",
    "file, model, n_outputs, replay_memory, best_weights, state, reward = initializer(env, N)\n",
    "\n",
    "camera = Camera(plt.figure())\n",
    "\n",
    "\n",
    "best_H   = 0\n",
    "Hamilton = []               # just for plot total hamilton per episode\n",
    "giant = []                  # Represents the percentage of members of the giant component \n",
    "\n",
    "# just for checking prolems\n",
    "all_q = []; next_q = []; qq = []; target_q = []; LOSS = []; grads = []; trainable_variables = []; \n",
    "for i in range(N): all_q.append(0); next_q.append(0); qq.append(0); target_q.append(0); LOSS.append(0); grads.append(0); trainable_variables.append(0)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "for episode in range(800):\n",
    "    \n",
    "    for _ in range(20):\n",
    "        epsilon = max(1 - episode/600, 0.0)             # first is more random and than use greedy\n",
    "        state, reward = play_one_step(env, N, model, state, n_outputs, replay_memory, epsilon)\n",
    "\n",
    "    if episode > 50:\n",
    "        for i in range(N):\n",
    "            (all_q[i], next_q[i], qq[i], target_q[i], LOSS[i],\n",
    "             grads[i], trainable_variables[i]) = SDS.training_step(i, model, n_outputs, replay_memory, batch_size, discount_rate)\n",
    "        log_print(file, N, env, episode, epsilon, reward, H, \n",
    "                  all_q, next_q, qq, target_q, LOSS, grads, trainable_variables)    \n",
    "    \n",
    "     \n",
    "    H = 0\n",
    "    for i in range(env.N): H += env.Hamiltonian(i)        # Hamiltonian of the whole system\n",
    "    Hamilton.append(H) \n",
    "    \n",
    "    if H <= best_H and episode > 500:                   # find the minimum of Hamiltonian\n",
    "        best_weight = model[0].get_weights()            # saving model weights for the best Hamiltonian\n",
    "        best_H = H\n",
    "        \n",
    "      \n",
    "    if episode%10 == 0:\n",
    "        giant.append( env.Plot(camera, episode) )\n",
    "        \n",
    "    print(\"\\rEpisode: {}, eps: {:.3f}, Min(Hamilton): {:.3f}, H: {:.3f},     R1: {:.3f}, R2: {:.3f}, R3: {:.3f}, R4: {:.3f}\"\n",
    "          .format(episode, epsilon, best_H, H, env.r[0], env.r[1], env.r[2], env.r[3]), end=\"\")\n",
    "    \n",
    "    \n",
    "anim = camera.animate(interval= 100, repeat=True, repeat_delay= 500, blit=True)\n",
    "anim.save('./result/animation_C.gif')\n",
    "file.close()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "print(best_H)\n",
    "Plot_H_G(episode, Hamilton, giant)\n",
    "\n",
    "\n",
    "\n",
    "# last_width = model[0].get_weights()\n",
    "# model[0].save('100agent.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b8b5a",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23eabb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639be20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
