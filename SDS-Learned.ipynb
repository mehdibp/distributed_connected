{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689bc112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import SDS_Environment as SDS\n",
    "import pandas as pd\n",
    "from SDS_Environment import np, plt, tf, Camera, nx\n",
    "\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019ff6b",
   "metadata": {},
   "source": [
    "# Fanctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e616808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initializer(env, N):\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    camera = Camera(plt.figure(figsize=(15,15)))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # build models\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    input_shape = [3]              # == env.observation_space.shape\n",
    "    n_outputs = 2                  # == env.action_space.n\n",
    "\n",
    "    model = []                     # make a model for each agent\n",
    "    for _ in range(N):\n",
    "        model.append(tf.keras.models.load_model('100agent.keras', \n",
    "                            custom_objects={'custom_activation': tf.keras.layers.Activation(custom_activation)}, \n",
    "                            compile=False))\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # initilaze states and rewards from environment class    \n",
    "    state, reward = env.step(np.zeros(N), 1)\n",
    "    state = [[state[j][i] for j in range(input_shape[0])] for i in range(N)]\n",
    "    \n",
    "    return camera, model, n_outputs, state\n",
    "    \n",
    "def custom_activation(x):\n",
    "    xx = (tf.keras.backend.softplus(x))**0.5\n",
    "    return -tf.keras.backend.elu(-xx + 100) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3b3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, model, N, state, episode):\n",
    "    action = np.zeros(env.N)\n",
    "\n",
    "    while N < env.N:\n",
    "        N += 1\n",
    "        model.append(tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=[len(state[0])]),\n",
    "                    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "                    tf.keras.layers.Dense(2, activation=custom_activation)\n",
    "                ]))\n",
    "                \n",
    "        model[-1] = tf.keras.models.load_model('100agent.keras', \n",
    "                            custom_objects={'custom_activation': tf.keras.layers.Activation(custom_activation)}, \n",
    "                            compile=False)\n",
    "\n",
    "    for i in range(env.N):\n",
    "        actionnnn = model[i].predict(np.reshape(state[i], (3)).reshape(1, -1), verbose=0)[0]\n",
    "        action[i] = np.argmax(actionnnn)\n",
    "    \n",
    "    next_state, reward = env.step((action-0.5)*2, episode)\n",
    "    next_state = [[next_state[j][i] for j in range(3)] for i in range(env.N)]\n",
    "\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0977f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Results(episode, Hamilton, Giant, Edges, Tau):\n",
    "    plt.figure(figsize=(16,12))\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Hamilton\")\n",
    "    plt.plot(Hamilton)\n",
    "    plt.text(0.7*episode, 1.1*max(Hamilton), \"Min(H): %f\" % (min(Hamilton)) )\n",
    "    plt.text(0.7*episode, 1.2*max(Hamilton), \"Arg(H): %f\" % (np.argmin(Hamilton)) )\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"Giant Component\")\n",
    "    plt.plot(Giant)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"Transition Range\")\n",
    "    plt.plot(Edges)\n",
    "    plt.grid(alpha=0.3) \n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(\"Tau\")\n",
    "    plt.plot(Tau)\n",
    "    plt.grid(alpha=0.3) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65258bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Result(env, P_i, Q_i, previous_):\n",
    "\n",
    "    # Hamiltonian of the whole system -----------------------------------------\n",
    "    hamilton = 0\n",
    "    for i in range(env.N): hamilton += env.Hamiltonian(i)\n",
    "    # -------------------------------------------------------------------------\n",
    "    edge  = env.k.sum()/2\n",
    "    # -------------------------------------------------------------------------\n",
    "    energy = (0.2*env.r**2).sum()\n",
    "\n",
    "    # Giant component of network (%) ------------------------------------------\n",
    "    G = nx.from_numpy_array(env.A)\n",
    "    giant = len((sorted(nx.connected_components(G), key=len, reverse=True))[0])/env.N * 100\n",
    "\n",
    "    # Calculate Tau = 1/sM sigma(P_ij/Q_ij)------------------------------------\n",
    "    for i in range(env.N):\n",
    "        for j in range(i, env.N):\n",
    "            if previous_[i][j] != env.A[i][j]:\n",
    "                Q_i[i][j] += 1\n",
    "                Q_i[j][i] += 1\n",
    "\n",
    "    P_i += env.A\n",
    "    previous_ = env.A\n",
    "    tau = 1/(1000* (env.N*(env.N-1)/2)) * (P_i/Q_i).sum()\n",
    "    \n",
    "    return hamilton, edge, energy, giant, tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b3855",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100                      # Number of agents\n",
    "L = 15                       # The length of the simulation box\n",
    "env = SDS.Distributed_System(N,L, flipping=True, on_off=False, moving=False)\n",
    "\n",
    "camera, model, n_outputs, state = Initializer(env, N)\n",
    "P_ij=np.copy(env.A); Q_ij=np.ones((env.N, env.N)); previous_A=np.zeros((env.N, env.N))\n",
    "\n",
    "Hamilton = []               # Just for plot total hamilton per episode\n",
    "Giant  = []                 # Represents the percentage of members of the giant component \n",
    "Edges  = []\n",
    "Energy = []\n",
    "Tau    = []\n",
    "\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = play_one_step(env, model, N, state, episode)\n",
    "    \n",
    "    \n",
    "    # Show Results --------------------------------------------------------------------------\n",
    "    hamilton, edge, energy, giant, tau = Calculate_Result(env, P_ij, Q_ij, previous_A)\n",
    "    Hamilton.append(hamilton)\n",
    "    Edges.append(edge)\n",
    "    Energy.append(energy)\n",
    "    Tau.append(tau)\n",
    "    Giant.append(giant)\n",
    "    \n",
    "    if episode%10 == 0: env.Plot(camera, episode)\n",
    "    \n",
    "    print(\"\\rEpisode: {}, H: {:.3f}, N: {:.1f}\".format(episode, hamilton, env.N), end=\"\")\n",
    "    \n",
    "        \n",
    "anim = camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=True)\n",
    "anim.save('./All result/animation_.gif')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "print(\"\\n\\n\", np.average(Hamilton))\n",
    "Plot_Results(episode, Hamilton, Giant, Energy, Tau)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "df = pd.DataFrame({'Hamilton': Hamilton, 'Giant': Giant, 'Edges': Edges, 'Energy': Energy, 'Tau': Tau})\n",
    "df.to_csv('./All result/csv/Results_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f790ee9",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e14c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50                      # Number of agents\n",
    "L = 8                       # The length of the simulation box\n",
    "env = SDS.Distributed_System(N,L)\n",
    "\n",
    "camera, model, n_outputs, state = initializer(env, N)\n",
    "\n",
    "Hamilton = []               # Just for plot total hamilton per episode\n",
    "Giant  = []                 # Represents the percentage of members of the giant component \n",
    "Edges  = []\n",
    "Energy = []\n",
    "R_average = []\n",
    "\n",
    "for episode in range(300):\n",
    "    action = np.zeros(N)\n",
    "    for i in range(env.N):\n",
    "        if env.k[i] < 5:\n",
    "            action[i] = np.random.randint(2)\n",
    "    \n",
    "    next_state, reward = env.step(action, episode)\n",
    "    next_state = [[next_state[j][i] for j in range(3)] for i in range(env.N)]\n",
    "    state = next_state\n",
    "      \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # Show Results\n",
    "    H = 0\n",
    "    for i in range(env.N): H += env.Hamiltonian(i)        # Hamiltonian of the whole system\n",
    "    Hamilton.append(H) \n",
    "    # Edges.append(env.k.sum()/2)\n",
    "    Energy.append((0.2*env.r**2).sum())\n",
    "    R_average.append(np.average(env.r))\n",
    "    \n",
    "    if episode%1 == 0:\n",
    "        Giant.append( env.Plot(camera, episode) )\n",
    "\n",
    "    print(\"\\rEpisode: {}, H: {:.3f}, N: {:.1f}\".format(episode, H, env.N), end=\"\")\n",
    "        \n",
    "        \n",
    "anim = camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=True)\n",
    "anim.save('./All result/animation_C2.gif')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "print(\"\\n\", min(Hamilton), np.argmin(Hamilton), R_average[np.argmin(Hamilton)])\n",
    "Plot_Results(episode, Hamilton, Giant, Energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b8b5a",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef884211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ec46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c2cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59520abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
