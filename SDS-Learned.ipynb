{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SDS_Environment as SDS\n",
    "from SDS_Environment import np, plt, tf, Camera\n",
    "\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e616808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(env, N):\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    camera = Camera(plt.figure(figsize=(15,15)))\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # build models\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    input_shape = [3]              # == env.observation_space.shape\n",
    "    n_outputs = 2                  # == env.action_space.n\n",
    "\n",
    "    model = []                     # make a model for each agent\n",
    "    for i in range(N):\n",
    "        model.append(tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "            tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "            tf.keras.layers.Dense(n_outputs, activation=custom_activation)\n",
    "        ]))\n",
    "        \n",
    "        model[i] = tf.keras.models.load_model('100agent.keras', \n",
    "                            custom_objects={'custom_activation': tf.keras.layers.Activation(custom_activation)}, \n",
    "                            compile=False)\n",
    "        \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # initilaze states and rewards from environment class    \n",
    "    state, reward = env.step(np.zeros(N), 1)\n",
    "    state = [[state[j][i] for j in range(input_shape[0])] for i in range(N)]\n",
    "    \n",
    "    return camera, model, n_outputs, state\n",
    "    \n",
    "def custom_activation(x):\n",
    "    xx = (tf.keras.backend.softplus(x))**0.5\n",
    "    return -tf.keras.backend.elu(-xx + 100) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3b3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, model, N, state, episode):\n",
    "    action = np.zeros(env.N)\n",
    "\n",
    "    while N < env.N:\n",
    "        N += 1\n",
    "        model.append(tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=[len(state[0])]),\n",
    "                    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "                    tf.keras.layers.Dense(2, activation=custom_activation)\n",
    "                ]))\n",
    "                \n",
    "        model[-1] = tf.keras.models.load_model('100agent.keras', \n",
    "                            custom_objects={'custom_activation': tf.keras.layers.Activation(custom_activation)}, \n",
    "                            compile=False)\n",
    "\n",
    "    for i in range(env.N):\n",
    "        actionnnn = model[i].predict(np.reshape(state[i], (3)).reshape(1, -1), verbose=0)[0]\n",
    "        action[i] = np.argmax(actionnnn)\n",
    "    \n",
    "    next_state, reward = env.step((action-0.5)*2, episode)\n",
    "    next_state = [[next_state[j][i] for j in range(3)] for i in range(env.N)]\n",
    "\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0977f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_H_G(episode, Hamilton, Giant, Edges):\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Hamilton\")\n",
    "    plt.plot(Hamilton)\n",
    "    plt.text(0.7*episode, 1.1*max(Hamilton), \"Min(H): %f\" % (min(Hamilton)) )\n",
    "    plt.text(0.7*episode, 1.2*max(Hamilton), \"Arg(H): %f\" % (np.argmin(Hamilton)) )\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"Giant Component\")\n",
    "    plt.plot(Giant)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(\"Tansition Range\")\n",
    "    plt.plot(Edges)\n",
    "    plt.grid(alpha=0.3) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100                      # Number of agents\n",
    "L = 15                       # The length of the simulation box\n",
    "env = SDS.Distributed_System(N,L)\n",
    "\n",
    "camera, model, n_outputs, state = initializer(env, N)\n",
    "\n",
    "Hamilton = []               # Just for plot total hamilton per episode\n",
    "Giant  = []                 # Represents the percentage of members of the giant component \n",
    "Edges  = []\n",
    "Energy = []\n",
    "\n",
    "\n",
    "for episode in range(800):\n",
    "    state = play_one_step(env, model, N, state, episode)\n",
    "    \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # Show Results\n",
    "    H = 0\n",
    "    for i in range(env.N): H += env.Hamiltonian(i)        # Hamiltonian of the whole system\n",
    "    Hamilton.append(H) \n",
    "    Edges.append(env.k.sum()/2)\n",
    "    Energy.append((0.2*env.r**2).sum())\n",
    "    \n",
    "    \n",
    "    if episode%10 == 0:\n",
    "        Giant.append( env.Plot(camera, episode) ) \n",
    "\n",
    "    print(\"\\rEpisode: {}, H: {:.3f}, N: {:.1f}\".format(episode, H, env.N), end=\"\")\n",
    "    \n",
    "        \n",
    "anim = camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=True)\n",
    "anim.save('./result/animation_Learned.gif')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "print(\"\\n\\n\", np.average(Hamilton))\n",
    "Plot_H_G(episode, Hamilton, Giant, Energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f790ee9",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e14c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50                      # Number of agents\n",
    "L = 8                       # The length of the simulation box\n",
    "env = SDS.Distributed_System(N,L)\n",
    "\n",
    "camera, model, n_outputs, state = initializer(env, N)\n",
    "\n",
    "Hamilton = []               # Just for plot total hamilton per episode\n",
    "Giant  = []                 # Represents the percentage of members of the giant component \n",
    "Edges  = []\n",
    "Energy = []\n",
    "R_average = []\n",
    "\n",
    "for episode in range(300):\n",
    "    action = np.zeros(N)\n",
    "    for i in range(env.N):\n",
    "        if env.k[i] < 5:\n",
    "            action[i] = np.random.randint(2)\n",
    "    \n",
    "    next_state, reward = env.step(action, episode)\n",
    "    next_state = [[next_state[j][i] for j in range(3)] for i in range(env.N)]\n",
    "    state = next_state\n",
    "      \n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    # Show Results\n",
    "    H = 0\n",
    "    for i in range(env.N): H += env.Hamiltonian(i)        # Hamiltonian of the whole system\n",
    "    Hamilton.append(H) \n",
    "    # Edges.append(env.k.sum()/2)\n",
    "    Energy.append((0.2*env.r**2).sum())\n",
    "    R_average.append(np.average(env.r))\n",
    "    \n",
    "    if episode%1 == 0:\n",
    "        Giant.append( env.Plot(camera, episode) )\n",
    "\n",
    "    print(\"\\rEpisode: {}, H: {:.3f}, N: {:.1f}\".format(episode, H, env.N), end=\"\")\n",
    "        \n",
    "        \n",
    "anim = camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=True)\n",
    "anim.save('./result/animation_C2.gif')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "print(\"\\n\", min(Hamilton), np.argmin(Hamilton), R_average[np.argmin(Hamilton)])\n",
    "Plot_H_G(episode, Hamilton, Giant, Energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b8b5a",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c2cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59520abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
