{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9629d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AgeNet import environments, physics, Agent, AgentsSimulator\n",
    "from AgeNet import analyses, experiments\n",
    "from AgeNet.learning import RLBrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af222b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment ------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "L = 14                                  # The length of the simulation box\n",
    "buildings_type = \"random\"               # Type of placement of buildings (random or regular)\n",
    "num_buildings  = 00                     # Number of buildings in environment\n",
    "num_streets    = 0                      # Number of streets in environment\n",
    "\n",
    "sumo_cfg    = \"./Sumo/SumoScenario/simulation.sumocfg\"\n",
    "sumo_binary = \"/nix/store/gp4hbfbrxg4zzjs7g3nl7vy0rw4jmm2y-sumo-1.22.0/bin/sumo\"\n",
    "step_length = 1.0                       # \n",
    "use_gui     = False                     # \n",
    "sumo_seed   = None                      # \n",
    "\n",
    "environment_parameters = [L, buildings_type, num_buildings, num_streets]            # manual env\n",
    "# environment_parameters = [sumo_cfg, sumo_binary, step_length, use_gui, sumo_seed]   # sumo env\n",
    "\n",
    "\n",
    "# 2. Agent Learning ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "N = 100                                 # Number of agents\n",
    "Alphas = [-0.5, 0.1, 0.2, -0.5]       # Hamiltonian constant coefficients\n",
    "\n",
    "state_dim     = 3                       # Dimensions of the agent state for neural network input\n",
    "action_dim    = 2                       # Neural network output dimensions (Q values)\n",
    "learning_rate = 1e-5                    # Learning rate in model training\n",
    "discount_rate = 0.98                    # The effect of future decisions on current decisions\n",
    "batch_size    = 20                      # Selected package of replay_memory to perform training\n",
    "steps_per_train = 10                    # The training function is called once every ... steps\n",
    "Initialize_model_path = f'./Trained Models/20260213-161716/model_minHamiltonian.keras'\n",
    "brain_parameters = [state_dim, action_dim, learning_rate, discount_rate, batch_size, Initialize_model_path]\n",
    "\n",
    "\n",
    "# 3. Communication ----------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "absorption  = 0.5\n",
    "base_delay  = 0.01\n",
    "max_delay   = 1.0\n",
    "communication_parameters = [absorption, base_delay, max_delay]\n",
    "\n",
    "\n",
    "# 4. Action Policy ----------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "action_policy_type = 'kNN'               # FR or kNN or MD or RL\n",
    "action_k_min = 5\n",
    "policy_parameters = [action_policy_type, action_k_min]\n",
    "\n",
    "\n",
    "# 5. Functions --------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "moving     = False                       # Enable the move function to agents\n",
    "requesting = False                       # Enable the sending request function\n",
    "training   = False                       # Enable each agent model training function\n",
    "Functions  = [moving, requesting, training]\n",
    "\n",
    "\n",
    "# 6. Directory and Seed -----------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "log_dir = f\"Results/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "seed = 30\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# Configs -------------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------\n",
    "config = {\n",
    "    \"environment\": environment_parameters,\n",
    "\n",
    "    'number of agent': N,\n",
    "    \"brain\": brain_parameters,\n",
    "    'alpha': Alphas,\n",
    "    'communication': communication_parameters,\n",
    "    'action policy': policy_parameters,\n",
    "    'functions': Functions,\n",
    "    'seed': seed\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a606987",
   "metadata": {},
   "source": [
    "### main runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8c7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, log_dir):\n",
    "    # Parameters ------------------------------------------------------------------------------------\n",
    "    N                        = config['number of agent']\n",
    "    environment_parameters   = config['environment']\n",
    "    brain_parameters         = config['brain']\n",
    "    Alphas                   = config['alpha']\n",
    "    communication_parameters = config['communication']\n",
    "    policy_parameters        = config['action policy']\n",
    "    Functions                = config['functions']\n",
    "    seed                     = config['seed']\n",
    "\n",
    "    # Init ------------------------------------------------------------------------------------------\n",
    "    np.random.seed(seed)\n",
    "    environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "\n",
    "    Agents = []\n",
    "    for i in range(N): \n",
    "        physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "        movement       = physics.NonMarkovian(environment)\n",
    "        agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement, policy_parameters)\n",
    "        Agents.append(agent)\n",
    "\n",
    "    simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "    # visualizer = analyses.NetworkVisualizer(environment)\n",
    "    result_exporter = experiments.ResultExporter(environment)\n",
    "    # result_ploter   = experiments.ResultPloter(log_dir, environment)\n",
    "\n",
    "\n",
    "    # Run -------------------------------------------------------------------------------------------\n",
    "    for step in range(1001):\n",
    "        simulator.run(Agents, steps_per_train=10)\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        result_exporter.collect(step, Agents)\n",
    "        # if step%5 == 0: visualizer.draw(Agents, step)\n",
    "        print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "    if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "    result_exporter.export_excel(f'./{log_dir}', f'{seed}, Policy Type = {policy_parameters[0]}, Requesting = {Functions[1]}')\n",
    "    # result_ploter.average_result(result_exporter.averaged_results)\n",
    "    # anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "    # anim.save(f'./{log_dir}/Animation.gif')\n",
    "    with open(f\"{log_dir}/config.yaml\", \"w\") as f: yaml.dump(config, f, sort_keys=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76205a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Functions_ = [ [True, False, False], [True, False, True], [True, True, True] ]  # [moving, requesting, training]\n",
    "Policy_Parameters_ = [ ['MD', 5], ['RL', 1], ['RL', 1] ]                        # [action_policy_type, action_k_min]\n",
    "log_dir = './kNN/Static'\n",
    "\n",
    "for seed in range(30, 50):\n",
    "    # for init in range(3):\n",
    "    #     Functions = Functions_[init]\n",
    "    #     policy_parameters = Policy_Parameters_[init]\n",
    "\n",
    "        config = config = {\n",
    "            \"environment\": environment_parameters,\n",
    "\n",
    "            'number of agent': N,\n",
    "            \"brain\": brain_parameters,\n",
    "            'alpha': Alphas,\n",
    "            'communication': communication_parameters,\n",
    "            'action policy': policy_parameters,\n",
    "            \n",
    "            'functions': Functions,\n",
    "            'seed': seed\n",
    "        }\n",
    "\n",
    "        main(config, log_dir)\n",
    "        print(f'\\t seed = {seed}, Policy Type = {policy_parameters[0]}, Requesting = {Functions[1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5dee0",
   "metadata": {},
   "source": [
    "### learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f0e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brain_parameters = [state_dim, action_dim, learning_rate, discount_rate, batch_size]\n",
    "brain = RLBrain(*brain_parameters)\n",
    "\n",
    "environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "\n",
    "Agents: list[Agent] = []\n",
    "for i in range(N): \n",
    "    physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "    movement       = physics.NonMarkovian(environment)\n",
    "    agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement)\n",
    "    agent.brain = brain\n",
    "    Agents.append(agent)\n",
    "\n",
    "Functions  = [False, False, training]\n",
    "simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "\n",
    "visualizer = analyses.NetworkVisualizer(environment)\n",
    "result_exporter = experiments.ResultExporter(environment)\n",
    "result_ploter   = experiments.ResultPloter(log_dir, environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3560403",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_H, best_G, best_H2G2 = 0, 0, 0\n",
    "for step in range(1001):\n",
    "    simulator.run(Agents, steps_per_train=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    result_exporter.collect(step, Agents)\n",
    "    if step%5 == 0: visualizer.draw(Agents, step)\n",
    "    print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "    # Find Best Weights ------------------------------------------------------------------------- \n",
    "    step, hamilton, giant, _, _, _, _ = result_exporter.averaged_results[-1]    \n",
    "    if step > 500:\n",
    "        if hamilton <= best_H:                                  # find the minimum of Hamiltonian\n",
    "            H_best_weight    = Agents[0].model.get_weights()    # saving model weights for the best Hamiltonian\n",
    "            best_H = hamilton\n",
    "\n",
    "        if giant >= best_G:\n",
    "            G_best_weight    = Agents[0].model.get_weights()    # saving model weights for the best Giant Component\n",
    "            best_G = giant\n",
    "\n",
    "        if np.sqrt(hamilton**2 + (giant/1e3)**2) >= best_H2G2:\n",
    "            H2G2_best_weight = Agents[0].model.get_weights()\n",
    "            best_H2G2 = np.sqrt(hamilton**2 + (giant/1e3)**2)\n",
    "\n",
    "\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "result_exporter.export_excel(f'./{log_dir}')\n",
    "result_ploter.average_result(result_exporter.averaged_results)\n",
    "anim.save(f'./{log_dir}/Animation - Training.gif')\n",
    "with open(f\"{log_dir}/config.yaml\", \"w\") as f: yaml.dump(config, f, sort_keys=False)\n",
    "\n",
    "\n",
    "last_weight = Agents[0].model.get_weights()\n",
    "\n",
    "best_models = [H_best_weight, G_best_weight, H2G2_best_weight, last_weight]\n",
    "best_models_name = ['minHamiltonian', 'maxConnectivity', 'balancedHG', 'FinalStep']\n",
    "for i, best_model in enumerate(best_models):\n",
    "    model_ = Agents[0].model\n",
    "    model_.set_weights(best_model)\n",
    "    model_.save(f'{log_dir}/model_{best_models_name[i]}.keras')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682accdc",
   "metadata": {},
   "source": [
    "### classical algorithem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "\n",
    "Agents = []\n",
    "for i in range(N): \n",
    "    physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "    movement       = physics.NonMarkovian(environment)\n",
    "    agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement, policy_parameters)\n",
    "    Agents.append(agent)\n",
    "\n",
    "Functions  = [moving, False, False]\n",
    "simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "\n",
    "visualizer = analyses.NetworkVisualizer(environment)\n",
    "result_exporter = experiments.ResultExporter(environment)\n",
    "result_ploter   = experiments.ResultPloter(log_dir, environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1001):\n",
    "    simulator.run(Agents)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    result_exporter.collect(step, Agents)\n",
    "    if step%5 == 0: visualizer.draw(Agents, step)\n",
    "    print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "result_exporter.export_excel(f'./{log_dir}')\n",
    "result_ploter.average_result(result_exporter.averaged_results)\n",
    "anim.save(f'./{log_dir}/Animation.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc984e",
   "metadata": {},
   "source": [
    "### Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "\n",
    "Agents = []\n",
    "for i in range(N): \n",
    "    physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "    movement       = physics.NonMarkovian(environment)\n",
    "    agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement)\n",
    "    Agents.append(agent)\n",
    "\n",
    "simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "\n",
    "visualizer = analyses.NetworkVisualizer(environment)\n",
    "result_exporter = experiments.ResultExporter(environment)\n",
    "result_ploter   = experiments.ResultPloter(log_dir, environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1001):\n",
    "    simulator.run(Agents, steps_per_train=10)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    result_exporter.collect(step, Agents)\n",
    "    if step%5 == 0: visualizer.draw(Agents, step)\n",
    "    print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "result_exporter.export_excel(f'./{log_dir}')\n",
    "result_ploter.average_result(result_exporter.averaged_results)\n",
    "anim.save(f'./{log_dir}/Animation.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913962b",
   "metadata": {},
   "source": [
    "### Sumo Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = environments.SumoEnvironment(*environment_parameters)\n",
    "environment.start()\n",
    "\n",
    "Agents = []\n",
    "\n",
    "simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "visualizer = analyses.NetworkVisualizer(environment)\n",
    "result_exporter = experiments.ResultExporter(environment)\n",
    "result_ploter   = experiments.ResultPloter(log_dir, environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cedd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: input. Received: the structure of inputs={'input': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000"
     ]
    }
   ],
   "source": [
    "ids, ids_prev = [], []\n",
    "\n",
    "for step in range(1001):\n",
    "    environment.step()\n",
    "    ids      = environment.get_entity_ids()\n",
    "    new_ids  = list(set(ids) - set(ids_prev))\n",
    "    del_ids  = list(set(ids_prev) - set(ids))\n",
    "    ids_prev = ids\n",
    "\n",
    "    for id in new_ids:\n",
    "        physical_state = physics.PhysicalState(environment, id)\n",
    "        agent = Agent(id, Alphas, brain_parameters, communication_parameters, physical_state)\n",
    "        Agents.append(agent)\n",
    "        simulator.add_agent(agent, Functions)\n",
    "\n",
    "    for id in del_ids:\n",
    "        removed_agent = next((agent for agent in Agents if agent.id == id), None)\n",
    "        Agents.remove(removed_agent)\n",
    "        simulator.remove_agent(removed_agent)\n",
    "    \n",
    "\n",
    "    simulator.run(Agents, steps_per_train=10)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    if len(ids) > 1:\n",
    "        result_exporter.collect(step, Agents)\n",
    "        if step%5 == 0: visualizer.draw(Agents, step)\n",
    "    print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "environment.close()\n",
    "if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "result_exporter.export_excel(f'./{log_dir}')\n",
    "result_ploter.average_result(result_exporter.averaged_results)\n",
    "anim.save(f'./{log_dir}/test.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106004e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbff1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(\n",
    "        environment_type:           str, \n",
    "        environment_parameters:     list,\n",
    "        brain_parameters:           list,\n",
    "        communication_parameters:   list = [0.5, 0.01, 1.0],\n",
    "        policy_parameters:          list = ['RL'],\n",
    "        Alphas:                     list = [-0.5, +0.3, 1.0, -1000],\n",
    "        Functions:                  list = [True, True, True],\n",
    "        N:                          int = 100,\n",
    "        log_dir:                    str = './'\n",
    "        \n",
    "        ):\n",
    "    \n",
    "    Agents = []\n",
    "\n",
    "    if environment_type == 'manual':\n",
    "        environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "        for i in range(N): \n",
    "            physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "            movement       = physics.NonMarkovian(environment)\n",
    "            agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement, policy_parameters)\n",
    "            Agents.append(agent)\n",
    "\n",
    "    elif environment_type == 'sumo':\n",
    "        environment = environments.SumoEnvironment(*environment_parameters)\n",
    "        environment.start()\n",
    "\n",
    "\n",
    "    simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "    visualizer = analyses.NetworkVisualizer(environment)\n",
    "    result_exporter = experiments.ResultExporter(environment)\n",
    "    result_ploter   = experiments.ResultPloter(log_dir, environment)\n",
    "\n",
    "    return environment, Agents, simulator, visualizer, result_exporter, result_ploter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(\n",
    "        environment_parameters:     list,\n",
    "        brain_parameters:           list,\n",
    "        communication_parameters:   list = [0.5, 0.01, 1.0],\n",
    "        policy_parameters:          list = ['RL'],\n",
    "        Alphas:                     list = [-0.5, +0.3, 1.0, -1000],\n",
    "        Functions:                  list = [True, True, True],\n",
    "        learning:                   bool = False,\n",
    "        N:                          int = 100,\n",
    "        log_dir:                    str = './'\n",
    "        \n",
    "        ):\n",
    "    \n",
    "    environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "    brain = RLBrain(*brain_parameters)\n",
    "\n",
    "    Agents = []\n",
    "    for i in range(N): \n",
    "        physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "        movement       = physics.NonMarkovian(environment)\n",
    "        agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement, policy_parameters)\n",
    "        if learning: agent.brain = brain\n",
    "        Agents.append(agent)\n",
    "\n",
    "\n",
    "    simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "    visualizer = analyses.NetworkVisualizer(environment)\n",
    "    result_exporter = experiments.ResultExporter(environment)\n",
    "    result_ploter   = experiments.ResultPloter(log_dir, environment)\n",
    "\n",
    "    return environment, Agents, simulator, visualizer, result_exporter, result_ploter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97499e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, log_dir):\n",
    "    N                        = config['number of agent']\n",
    "    environment_parameters   = config['environment']\n",
    "    brain_parameters         = config['brain']\n",
    "    Alphas                   = config['alpha']\n",
    "    communication_parameters = config['communication']\n",
    "    policy_parameters        = config['action policy']\n",
    "    Functions                = config['functions']\n",
    "    seed                     = config['seed']\n",
    "\n",
    "    # Init -------------------------------------------------------------------------------------------\n",
    "    np.random.seed(seed)\n",
    "    environment = environments.SimpleEnvironment(*environment_parameters)\n",
    "\n",
    "    Agents = []\n",
    "    for i in range(N): \n",
    "        physical_state = physics.PhysicalState(environment, f'{i}')\n",
    "        movement       = physics.NonMarkovian(environment)\n",
    "        agent = Agent(f'{i}', Alphas, brain_parameters, communication_parameters, physical_state, movement, policy_parameters)\n",
    "        Agents.append(agent)\n",
    "\n",
    "    simulator  = AgentsSimulator(environment, Agents, Functions)\n",
    "    visualizer = analyses.NetworkVisualizer(environment)\n",
    "    result_exporter = experiments.ResultExporter(environment)\n",
    "    result_ploter   = experiments.ResultPloter(log_dir, environment)\n",
    "\n",
    "\n",
    "    # Run -------------------------------------------------------------------------------------------\n",
    "    for step in range(1001):\n",
    "        simulator.run(Agents, steps_per_train=10)\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        result_exporter.collect(step, Agents)\n",
    "        if step%5 == 0: visualizer.draw(Agents, step)\n",
    "        print(\"\\rStep: {}\".format(step), end=\"\")\n",
    "\n",
    "    if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
    "\n",
    "    result_exporter.export_excel(f'./{log_dir}')\n",
    "    result_ploter.average_result(result_exporter.averaged_results)\n",
    "    anim = visualizer.camera.animate(interval= 120, repeat=True, repeat_delay= 500, blit=False)\n",
    "    anim.save(f'./{log_dir}/Animation.gif')\n",
    "    with open(f\"{log_dir}/config.yaml\", \"w\") as f: yaml.dump(config, f, sort_keys=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5bd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_binary = \"/nix/store/gp4hbfbrxg4zzjs7g3nl7vy0rw4jmm2y-sumo-1.22.0/bin/sumo\"\n",
    "env = environments.SumoEnvironment(\"./Sumo/SumoScenario/simulation.sumocfg\", sumo_binary, use_gui=False)\n",
    "env.start()\n",
    "\n",
    "T = 2001\n",
    "\n",
    "for t in range(T):\n",
    "    env.step()\n",
    "    ids = env.get_entity_ids()\n",
    "    for vid in ids:\n",
    "        pos = env.get_position(vid)\n",
    "    print(t, '\\t', ids)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf9c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.38905609893065"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_H = -50\n",
    "\n",
    "x = np.clip(-delta_H / 4, -20, 2)  # flip\n",
    "x = np.clip(-delta_H * 4, -20, 2)  # send request\n",
    "p = np.exp(x)\n",
    "p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8747fd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  8 20:47:30 2026    ./Saved Model/cProFile/cProFile.prof\n",
      "\n",
      "         2252379687 function calls (2174567748 primitive calls) in 4202.339 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 2690 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 10885000  744.331    0.000  744.353    0.000 {built-in method tensorflow.python._pywrap_tfe.TFE_Py_FastPathExecute}\n",
      "  2804100    3.724    0.000  412.780    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:96(convert_to_tensor_v2_with_dispatch)\n",
      "  2804100    4.246    0.000  408.998    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:166(convert_to_tensor_v2)\n",
      "  2181200    2.791    0.000  302.939    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2374(_dense_var_to_tensor)\n",
      "  2181200    3.407    0.000  300.126    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1612(_dense_var_to_tensor)\n",
      "  2181200   12.642    0.000  293.330    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:651(value)\n",
      "     3003   19.462    0.006  284.667    0.095 e:\\Projects\\MSc\\VaNet\\Distributed_Agent\\Functions.py:51(ExportResults)\n",
      "  2181200   10.846    0.000  263.438    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:813(_read_variable_op)\n",
      "  9646500   21.140    0.000  242.699    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:114(convert_to_tensor)\n",
      " 14372801   29.371    0.000  236.355    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\matplotlib\\path.py:99(__init__)\n",
      " 29729700  118.458    0.000  219.627    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2383(norm)\n",
      "170638684/170625142   91.225    0.000  219.355    0.000 {built-in method builtins.isinstance}\n",
      "  2181200    9.045    0.000  192.482    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:828(read_and_set_handle)\n",
      "  3641800   26.993    0.000  177.101    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:281(result_type)\n",
      "  1020800    1.870    0.000  173.941    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:391(numpy)\n",
      "  2181200    6.912    0.000  170.594    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:562(read_variable_op)\n",
      "   100100   38.722    0.000  141.168    0.001 e:\\Projects\\MSc\\VaNet\\Distributed_Agent\\Agent.py:63(update_neighbors)\n",
      "      300    0.011    0.000  140.233    0.467 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:24(clear_session)\n",
      "      300  140.062    0.467  140.076    0.467 {built-in method gc.collect}\n",
      "93997032/92914632  125.237    0.000  139.800    0.000 {built-in method builtins.getattr}\n",
      "   741100    3.149    0.000  130.310    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:541(subtract)\n",
      "981495/512377   13.296    0.000  129.511    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3862(add)\n",
      "  1202800   18.180    0.000  127.594    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1558(__init__)\n",
      "        2    0.000    0.000  127.268   63.634 d:\\Applications\\Anaconda3\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:373(save)\n",
      "        2    0.000    0.000  127.268   63.634 d:\\Applications\\Anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:279(save_workbook)\n",
      "        2    0.000    0.000  127.252   63.626 d:\\Applications\\Anaconda3\\Lib\\site-packages\\openpyxl\\writer\\excel.py:273(save)\n",
      "   741100    2.774    0.000  124.320    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13509(sub)\n",
      " 10502300   16.174    0.000  117.320    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:91(flatten)\n",
      "   720600   10.799    0.000  108.578    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3421(matmul)\n",
      " 10502300   12.082    0.000  101.146    0.000 d:\\Applications\\Anaconda3\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:62(flatten)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1b799ddb500>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "\n",
    "cProfile = './Saved Model/cProFile/cProFile.prof'\n",
    "p = pstats.Stats(cProfile)\n",
    "p.sort_stats(\"cumulative\").print_stats(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
